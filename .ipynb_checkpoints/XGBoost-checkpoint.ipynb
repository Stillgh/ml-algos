{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224a9564",
   "metadata": {},
   "source": [
    "XGBoost\n",
    "\n",
    "Regression\n",
    "Начало такое же как и в градиентом бустинге\n",
    "Изначальный лист делаем и строим дерево на residuals\n",
    "Но строится это дерево по другому\n",
    "Каждое дерево появляется как лист в который засовываем все residuals\n",
    "Просчитываем Quality/Similarity score для них\n",
    "SS = sum of residuals squared / number of residuals + lambda (параметр для регуляризации)\n",
    "Негативные и позитивные residuals обнулят друг друга (в отличии от squared sum of residuals)\n",
    "Подсчитали SS score\n",
    "Теперь мы должны узнать сможем ли мы do a better job clustering similar residuals if we split them into two groups\n",
    "Берём фичу (кол-во доз например)\n",
    "Берём первые два observations  (например 10 и 20)\n",
    "Берём среднее у них (15)\n",
    "Строим ноду dosage < 15 \n",
    "И засовываем в листья residuals по всем экземплярам (то есть в левый лист попадут residuals (y-15) тех экземпляров что < 15, в правый >=\n",
    "Теперь считаем SS score для этих двух листьев\n",
    "Получили ноду с двумя листьями  в которых (и ноде и ее листьях) находятся residuals и подсчитали ss score для этих трёх элементов\n",
    "(Когда residuals в ноде/листе сильно отличаются друг от друга, то они cancel each other out и ss score относительно мал, когда в ноде/листе residuals похожи или он там один, то ss score относительно большой)\n",
    "Теперь нужно квантифицировать насколько лучше листья cluster similar residuals than the root\n",
    "Делаем это вычислением Gain (gain of spkitting residuals into 2 groups\n",
    "Gain = SS левого листа +  SS правого - SS рута\n",
    "Теперь высчитываем SS и Gain для остальных значений которые мы используем в кач-ве thteshold в ноде (среднее каждых двух соседних значений dosage, те как когда выбираем какое значение continuous фичи выбрать в качестве трешхолда для ноды)\n",
    "Выбираем трешхолд который нам выдал максимальный Gain\n",
    "Когда все значения прошли (а если их супер много то как отсеивать?) и выбрали дерево с самым высоким Gain, то если в листе один residual всего, то не трогаем этот лист, если больше , то продолжаем делить\n",
    "Опять выбираем трешхолд для фичи (значение dosage с которым сравнивать будем) и строим дерево (точнее просто добавляем ещё один уровень в наше дерево)\n",
    "И также подсчитываем SS и Gain\n",
    "И так строим обычно до глубины в 6 деревьев, либо по параметру Cover - минимальное кол-во residuals в каждом листе\n",
    "Cover = знаменатель SS score минус лямбда (то есть number of residuals + lambda - lambda)\n",
    "\n",
    "Теперь пруним дерево на основании значений Gain\n",
    "Берём число (гамма тут называется ), например 130\n",
    "Берём Gain самой низкой ветки дерева и вычитаем из него гамму, если получилось негативное число , то удаляем ветку (в теории все дерево может так удалиться)\n",
    "\n",
    "Теперь как output value (y которое дерево выдает) листа высчитывается\n",
    "OV = sum of residuals/number of residuals + lambda\n",
    "Те как SS scire, но числитель в квадрат не возводится \n",
    "\n",
    "Все, построили первое дерево\n",
    "Делать предсказания также как и в обычном градиентном бустинге\n",
    "Начальное предсказание (начальный лист) + предсказание дерева умноженное на learning rate\n",
    "\n",
    "И также строим новые деревья (обучая их на новых residuals) пока residuals не станут супер мелкими или не достигнем максимального кол-ва деревьев\n",
    "\n",
    "\n",
    "Classification\n",
    "Сначала также делаем изначальное предсказание (initial prediction)\n",
    "По дефолту 0.5 ( вероятность что экземпляр относится к классу А, а не Б)\n",
    "У SS score новая формула\n",
    "Сумма ряда residuals возведённая в квадрат делить на\n",
    "Сумма ряда предыдущая вероятность умноженная на (1-предыдущая вероятность) + лямбда (регуляризационный параметр)\n",
    "Строим дерево\n",
    "Также берём лист в который все residuals засовываем (1-0.5, если реальный Y класс А, 0-0.5, если Б)\n",
    "Считаем SS score для этого листа\n",
    "Теперь должны узнать сможем ли мы лучше кластеризовать (cluster) похожие residuals ,если разделим из на две группы\n",
    "Также выбираем лучший (с наибольшим Gain) трешхолд по Х\n",
    "Короч все также как и в регрессии\n",
    "Cover тут то есть = Сумма ряда предыдущая вероятность умноженная на (1-предыдущая вероятность)\n",
    "Pruning такой же\n",
    "Предикты также делаются (+ также как и в обычном градиентном бустинге для классификации мы должны конвертить вероятность в log(odds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fdbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
