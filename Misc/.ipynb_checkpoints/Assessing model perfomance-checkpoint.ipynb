{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0c14e9",
   "metadata": {},
   "source": [
    "Assessing model perfomance\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "True positives, false positives, true negatives, false negatives\n",
    "\n",
    "Sensitivity - TP / (TP+FN)\n",
    "\n",
    "Specificity - T / (TN+FP)\n",
    "\n",
    "Precision - TP / (TP+FP)\n",
    "\n",
    "Recall == Sensitivity \n",
    "\n",
    "True Positive rate == recall == sensitivity\n",
    "\n",
    "False positive rate - FP / (FP+TN)\n",
    "\n",
    "ROC - граф для выбора оптимального трешхолда в алгоритмах классификации , по оси Y - True Positive Rate, по оси X - False positive rate, перебирая (как? Град спуск??) Различные трешхолды рисуем на графике значение TPr и FPr и в конце выбираем тот трешхолд который нам подходит\n",
    "\n",
    "AUC (area under curve) - когда хотим сравнить перфоманс разных классификационных моделей ,то чтобы не сравнивать ROC (ТК это муторно), сравниваем площади под ROC для каждой модели, у кого значение больше та модель и лучше \n",
    "Precision recall graph - если датасет несбалансирован (какогото типа Y больше), то лучше использовать precision recall graph, у него по оси X precision, вместо FPr, а ось Y переименована в recall. Он лучше работает на несбалансированных, ТК precision не включает true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afad1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
