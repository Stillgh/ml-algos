{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9132d1b",
   "metadata": {},
   "source": [
    "Decision trees\n",
    "\n",
    "Classification trees\n",
    "\n",
    "Сначала должны выбрать корневую ноду\n",
    "\n",
    "Для этого найдем фичу которая лучше всех предсказывает y\n",
    "\n",
    "Для категорийных (чисто бинарных походу) переменных используем Gini Impurity (есть и другие методы как Entropy и Information gain)\n",
    "Суть в том что берём фичу (любит кока колу) по которой пытаемся предсказать y (пойдет в кино или нет) и считаем сколько человек пошло и не пошло в кино и любят колу и пошло и не пошло в кино и НЕ любят колу\n",
    "\n",
    "Чем чище разделение получается (типо в левый лист попали только те кто пошел в кино, а в правый только те кто не пошел в кино), тем лучше \n",
    "\n",
    "Для continuous фичей мы сначала сортируем их , например фича возраст - 10,10, 12, 22, 33,44,55,66 и ТД\n",
    "\n",
    "У соседних берём среднее - 10, 11.5, 17 и тп\n",
    "\n",
    "И подсчитываем Giny Impurity для каждого из этих средних, типо нода проверяющая меньше ли возраст 10 и пошел ли чел в кино, потом также для 11.5 и для всех средних. В конце выбираем тот возраст для которого giny impurity индекс оказался наилучшим \n",
    "\n",
    "После того как рут создали, то смотрим на его детей - если там все под одну категорию попадают, то все,это лист будет. Если нет, то опять для других фичей подсчитываем giny impurity (теперь уже на тех экземплярах, что попали в эту ноду)\n",
    "\n",
    "\n",
    "Как делается предсказание - экземпляр прогоняется через дерево и попадает в лист, тот класс кол-во которого в листе будет больше и окажется нашим предсказанием для экземпляра\n",
    "\n",
    "\n",
    "С переобучением боримся с пом pruning либо мин кол-во экземпляров в листе \n",
    "\n",
    "Regression trees\n",
    "\n",
    "В этих деревьях предсказание делается не по тому классу, число экземпляров которого, больше в листе, а по среднему значению Y всех экземпляров в листе\n",
    "\n",
    "Для continuous фичей делаем также -\n",
    "\n",
    "например фича доход, предсказываем стоимость машины \n",
    "\n",
    "сортируем, находим среднее у соседних значений (это будет вариантом для разделения в ноде), потом вычисляем средний Y у всех экземпляров что слева от разделения и справа ( типо в среднем те кто зарабатывал меньше 10к владели тачкой за 5к, а те кто больше - за 20к) Находим SSR и так для каждой из возможных пар значений фичи и выбираем разделение с наименьшим SSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a6f02f",
   "metadata": {},
   "source": [
    "Missing data - \n",
    "\n",
    "мы можем найти фичу которая сильнее всего коррелирует с фичей в которой отсутствует значение\n",
    "\n",
    "Например фича А сильнее всего коррелирует с фичей Б\n",
    "\n",
    "Тогда берем значение фичи А у экземпляра с отсутствующим значением по фиче Б и копируем значение из того экземпляра где есть и А и Б (таких много так что моду берем) это для категориальной фичи\n",
    "\n",
    "Для continuous фичей мы строим график Ох - А, Оу - Б, и линейной регрессией определяем значение Б для значения А у экземпляра с отсутствующим значением Б (типо А - рост, Б - вес, обучаем линейную регрессию на тех экземплярах где естьи А и Б и подсчитываем ей значение для Б)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4391e5c",
   "metadata": {},
   "source": [
    "Pruning\n",
    "\n",
    "Это обрезка дерева, для борьбы с оверфиттингом тож юзается\n",
    "\n",
    "Разные виды бывают, популярный - cost complexity tree\n",
    "\n",
    "Для регрессионных деревьев\n",
    "\n",
    "Построили дерево - посчитали для него SSR\n",
    "\n",
    "потом снизу обрезали на поддерево (лист получается обрезаем), подсчитали SSR для обрезанного\n",
    "\n",
    "и так пока до корня не дойдем\n",
    "\n",
    "Выбираем лучшее на основании формулы\n",
    "Tree Score = SSR + a*T, где a - гиперпараметр (кросс валидацией подбирается), T - кол-во листьев в дереве\n",
    "\n",
    "\n",
    "Для классификационных заменяем SSR на Giny Impurity/Entropy/Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44855a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
